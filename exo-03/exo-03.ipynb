{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# RUNME ONCE\n",
    "# Run this cell once in order to install the dependencies needed for the lab. \n",
    "# After running this cell, remember to restart the kernel and proceed executing the next cells.\n",
    "################################################################################################\n",
    "\n",
    "! pip3 install gymnasium"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q* Learning \n",
    "In this notebook, we'll implement an agent that plays FrozenLake.\n",
    "The goal of this game is <b>to go from the starting state (S) to the goal state (G)</b> by walking only on frozen tiles (F) and avoid holes (H).However, the ice is slippery, <b>so you won't always move in the direction you intend (stochastic environment)</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 3 libraries:\n",
    "- `Numpy` for our Qtable\n",
    "- `Gymnasium` for our FrozenLake Environment. Read the documentation [here](https://gymnasium.farama.org/).\n",
    "- `Random` to generate random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import random\n",
    "from typing import NamedTuple\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create the environment\n",
    "- Here we'll create the FrozenLake environment. \n",
    "- OpenAI Gym is a library <b> composed of many environments that we can use to train our agents.</b>\n",
    "- In our case we choose to use Frozen Lake.\n",
    "- Note that S is the subject, F is frozen, H is the hole, and G the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params(NamedTuple):\n",
    "    total_episodes: int  # Total episodes\n",
    "    learning_rate: float  # Learning rate\n",
    "    max_steps: int\n",
    "    gamma: float  # Discounting rate\n",
    "    epsilon: float  # Exploration probability\n",
    "    map_size: int  # Number of tiles of one side of the squared environment\n",
    "    seed: int  # Define a seed so that we get reproducible results\n",
    "    is_slippery: bool  # If true the player will move in intended direction with probability of 1/3 else will move in either perpendicular direction with equal probability of 1/3 in both directions\n",
    "    n_runs: int  # Number of runs\n",
    "    action_size: int  # Number of possible actions\n",
    "    state_size: int  # Number of possible states\n",
    "    proba_frozen: float  # Probability that a tile is frozen\n",
    "    decay_rate: float\n",
    "\n",
    "params = Params(\n",
    "    total_episodes=2000,\n",
    "    learning_rate=0.8,\n",
    "    max_steps=99,\n",
    "    gamma=0.95,\n",
    "    epsilon=0.1,\n",
    "    map_size=4,\n",
    "    seed=123,\n",
    "    is_slippery=False,\n",
    "    n_runs=20,\n",
    "    action_size=None,\n",
    "    state_size=None,\n",
    "    proba_frozen=0.9,\n",
    "    decay_rate=0.005\n",
    ")\n",
    "print(params)\n",
    "\n",
    "# Set the seed\n",
    "rng = np.random.default_rng(params.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"FrozenLake-v1\",\n",
    "    is_slippery=params.is_slippery,\n",
    "    render_mode=\"rgb_array\",\n",
    "    desc=generate_random_map(\n",
    "        size=params.map_size, p=params.proba_frozen, seed=params.seed\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the Q-table and initialize it\n",
    "- Now, we'll create our Q-table, to know how much rows (states) and columns (actions) we need, we need to calculate the action_size and the state_size\n",
    "- OpenAI Gym provides us a way to do that: `env.action_space.n` and `env.observation_space.n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = params._replace(action_size=env.action_space.n)\n",
    "params = params._replace(state_size=env.observation_space.n)\n",
    "\n",
    "print(f\"Action size: {params.action_size}\")\n",
    "print(f\"State size: {params.state_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code to create the qtable from the action_size and state_size\n",
    "qtable = ####################\n",
    "print(qtable)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: The Q learning algorithm\n",
    "- Now we implement the Q learning algorithm:\n",
    "<img src=\"qtable_algo.png\" alt=\"Q algo\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 --> Q-values are already initialized.\n",
    "\n",
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# Step 2 --> For life or until learning is stopped ...\n",
    "for episode in range(params.total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(params.max_steps):\n",
    "        # Step 3 --> Choose an action (a) in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = ####################\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = # you can use the function argmax from numpy\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = # do something\n",
    "\n",
    "        # Step 4 --> Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = # use the method step from env. Check gym library documentation.\n",
    "\n",
    "        # Step 5 --> Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = # update following Bellman's equation\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Reduce epsilon \n",
    "    epsilon = params.min_epsilon + (params.max_epsilon - params.min_epsilon)*np.exp(-params.decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "    # Why do we need to reduce the epsilon? Comment below:\n",
    "    '''\n",
    "    Write your answer here:\n",
    "    \n",
    "\n",
    "    '''\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/params.total_episodes))\n",
    "print(qtable)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the trend of the evolution reward in each step. Your result should follow the trend as in the following figure:\n",
    "\n",
    "<img src=\"output.png\" alt=\"output\"/>\n",
    "\n",
    "You can check also the following [link](https://ai.stackexchange.com/questions/34071/is-the-optimal-policy-the-one-with-the-highest-accumulative-reward-q-learning-v) for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here your code to evaluate the evolution of the reward in each step.\n",
    "# You can copy the entire code for training or modify in the previous cell.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "Change the hyperparameters and comment about the impact of the map_size, and those related to the epsilon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
