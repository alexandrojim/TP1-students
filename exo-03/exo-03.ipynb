{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# RUNME ONCE\n",
    "# Run this cell once in order to install the dependencies needed for the lab.\n",
    "# After running this cell, remember to restart the kernel and proceed executing the next cells.\n",
    "################################################################################################\n",
    "\n",
    "! pip3 install gymnasium pygame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q* Learning \n",
    "In this notebook, we'll implement an agent that plays FrozenLake.\n",
    "The goal of this game is <b>to go from the starting state (S) to the goal state (G)</b> by walking only on frozen tiles (F) and avoid holes (H).However, the ice is slippery, <b>so you won't always move in the direction you intend (stochastic environment)</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 3 libraries:\n",
    "- `Numpy` for our Qtable\n",
    "- `Gymnasium` for our FrozenLake Environment. Read the documentation [here](https://gymnasium.farama.org/).\n",
    "- `Random` to generate random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import random\n",
    "from typing import NamedTuple\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create the environment\n",
    "- Here we'll create the FrozenLake environment. \n",
    "- OpenAI Gym is a library <b> composed of many environments that we can use to train our agents.</b>\n",
    "- In our case we choose to use Frozen Lake.\n",
    "- Note that S is the subject, F is frozen, H is the hole, and G the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params(NamedTuple):\n",
    "    total_episodes: int  # Total episodes\n",
    "    learning_rate: float  # Learning rate\n",
    "    max_steps: int\n",
    "    gamma: float  # Discounting rate\n",
    "    epsilon: float  # Exploration probability\n",
    "    max_epsilon: float # Exploration probability at start\n",
    "    min_epsilon: float # Minimum exploration probability \n",
    "    map_size: int  # Number of tiles of one side of the squared environment\n",
    "    seed: int  # Define a seed so that we get reproducible results\n",
    "    is_slippery: bool  # If true the player will move in intended direction with probability of 1/3 else will move in either perpendicular direction with equal probability of 1/3 in both directions\n",
    "    n_runs: int  # Number of runs\n",
    "    action_size: int  # Number of possible actions\n",
    "    state_size: int  # Number of possible states\n",
    "    proba_frozen: float  # Probability that a tile is frozen\n",
    "    decay_rate: float\n",
    "\n",
    "params = Params(\n",
    "    total_episodes=4000,\n",
    "    learning_rate=0.8,\n",
    "    max_steps=99,\n",
    "    gamma=0.95,\n",
    "    epsilon=1,\n",
    "    max_epsilon = 1.0,\n",
    "    min_epsilon = 0.01,\n",
    "    map_size=4,\n",
    "    seed=123,\n",
    "    is_slippery=False,\n",
    "    n_runs=20,\n",
    "    action_size=None,\n",
    "    state_size=None,\n",
    "    proba_frozen=0.9,\n",
    "    decay_rate=0.001\n",
    ")\n",
    "print(params)\n",
    "\n",
    "# Set the seed\n",
    "rng = np.random.default_rng(params.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"FrozenLake-v1\",\n",
    "    is_slippery=params.is_slippery,\n",
    "    render_mode=\"ansi\",\n",
    "    desc=generate_random_map(\n",
    "        size=params.map_size, p=params.proba_frozen, seed=params.seed\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the Q-table and initialize it\n",
    "- Now, we'll create our Q-table, to know how much rows (states) and columns (actions) we need, we need to calculate the action_size and the state_size\n",
    "- OpenAI Gym provides us a way to do that: `env.action_space.n` and `env.observation_space.n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = params._replace(action_size=env.action_space.n)\n",
    "params = params._replace(state_size=env.observation_space.n)\n",
    "\n",
    "print(f\"Action size: {params.action_size}\")\n",
    "print(f\"State size: {params.state_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code to create the qtable from the action_size and state_size\n",
    "qtable = #######################\n",
    "print(qtable)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: The Q learning algorithm\n",
    "- Now we implement the Q learning algorithm:\n",
    "<img src=\"qtable_algo.png\" alt=\"Q algo\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 --> Q-values are already initialized.\n",
    "\n",
    "rewards = [] # List of rewards\n",
    "steps = [] # List of steps\n",
    "e_progress = []\n",
    "\n",
    "# Step 2 --> For life or until learning is stopped ...\n",
    "for episode in range(params.total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()[0]\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(params.max_steps):\n",
    "        # Step 3 --> Choose an action (a) in the current world state (s)\n",
    "        ## First we randomize a number between 0 and 1 following a uniform distribution.\n",
    "        exp_exp_tradeoff = ####################\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > params.epsilon:\n",
    "            action = # you can use the function argmax from numpy\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Step 4 --> Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # Step 5 --> Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] =  # update following Bellman's equation\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        done = truncated or terminated\n",
    "        if done: \n",
    "            break\n",
    "\n",
    "    steps.append(step + 1)\n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "    # Reduce epsilon \n",
    "    params = params._replace(epsilon=params.min_epsilon + (params.max_epsilon - params.min_epsilon)*np.exp(-params.decay_rate*episode))\n",
    "    e_progress.append(params.epsilon)\n",
    "\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/params.total_episodes))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display images\n",
    "import matplotlib.pyplot as plt\n",
    "def moving_average(data, window_size=50):\n",
    "    \"\"\"Compute moving average of a given list using specified window size.\"\"\"\n",
    "    cumsum = [0]\n",
    "    for i, x in enumerate(data, 1):\n",
    "        cumsum.append(cumsum[i-1] + x)\n",
    "        if i >= window_size:\n",
    "            moving_avg = (cumsum[i] - cumsum[i-window_size]) / window_size\n",
    "            yield moving_avg\n",
    "        else:\n",
    "            yield cumsum[i] / i\n",
    "\n",
    "fig, axarr = plt.subplots(3, 2, figsize=(15, 15)) \n",
    "\n",
    "# Histogram for rewards\n",
    "n_rewards, bins_rewards, patches_rewards = axarr[0][0].hist(rewards, bins=[-0.5, 0.5, 1.5], edgecolor='black', align='mid', density=False)\n",
    "axarr[0][0].set_title('Rewards per episode histogram')\n",
    "axarr[0][0].set_xticks([0, 1])  # Set x-axis ticks for 0 and 1\n",
    "\n",
    "# Annotate rewards histogram with counts\n",
    "for count, patch in zip(n_rewards, patches_rewards):\n",
    "    height = patch.get_height()\n",
    "    axarr[0][0].text(patch.get_x() + patch.get_width() / 2, height + 0.1, int(count),\n",
    "                     ha='center', va='bottom')\n",
    "\n",
    "# Generate bin edges for steps histogram centered around integers\n",
    "step_min = int(np.floor(min(steps)))\n",
    "step_max = int(np.ceil(max(steps)))\n",
    "bins_steps_centered = np.arange(step_min - 0.5, step_max + 1.5, 1)\n",
    "\n",
    "# Histogram for steps\n",
    "n_steps, _, patches_steps = axarr[0][1].hist(steps, bins=bins_steps_centered, edgecolor='black', align='mid', density=True)\n",
    "axarr[0][1].set_title('Steps per episode histogram')\n",
    "axarr[0][1].set_yscale('log')\n",
    "\n",
    "\n",
    "# # Annotate steps histogram with counts\n",
    "# for count, patch in zip(n_steps, patches_steps):\n",
    "#     height = patch.get_height()\n",
    "#     axarr[0][1].text(patch.get_x() + patch.get_width() / 2, height + 0.1, int(count),\n",
    "#                      ha='center', va='bottom')\n",
    "\n",
    "# Plot for cumulative rewards\n",
    "axarr[1][0].plot(np.cumsum(rewards)/params.total_episodes)\n",
    "axarr[1][0].set_title('Normalized cumulative rewards')\n",
    "axarr[1][0].grid(True)\n",
    "\n",
    "\n",
    "smoothed_steps = list(moving_average(steps))\n",
    "# axarr[1][1].plot(np.cumsum([a/b for a, b in zip(rewards, np.cumsum(steps))]))  \n",
    "axarr[1][1].plot(steps, label='Original Steps', alpha=0.5)  \n",
    "axarr[1][1].plot(smoothed_steps, label='Smoothed Steps', linewidth=2)\n",
    "axarr[1][1].set_title('Steps per episode')\n",
    "axarr[1][1].set_xlabel('episodes')\n",
    "axarr[1][1].set_ylabel('Average steps per episode')\n",
    "axarr[1][1].grid(True)\n",
    "axarr[1][1].legend()\n",
    "\n",
    "\n",
    "axarr[2][0].plot(e_progress)\n",
    "axarr[2][0].set_title('E-greedy algorithm')\n",
    "axarr[2][0].set_xlabel('episodes')\n",
    "axarr[2][0].set_ylabel('epsilon')\n",
    "axarr[2][0].grid(True)\n",
    "\n",
    "\n",
    "smoothed_rewards = list(moving_average(rewards))\n",
    "axarr[2][1].plot(rewards, label='Original Rewards', alpha=0.5)  \n",
    "axarr[2][1].plot(smoothed_rewards, label='Smoothed Rewards', linewidth=2)\n",
    "axarr[2][1].set_title('Q learning algorithm')\n",
    "axarr[2][1].set_xlabel('episodes')\n",
    "axarr[2][1].set_ylabel('reward per episode')\n",
    "axarr[2][1].grid(True)\n",
    "axarr[2][1].legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "1. Change the hyperparameters and comment about the impact of the map_size, and those related to the epsilon. \n",
    "2. Why do we need to reduce the epsilon? What happens if we don't modify it?\n",
    "3. What happen with the score and Qtable when you run the training several times? Why?\n",
    "4. Using the plots above, comment each of them (what do they mean) and what do you observe when you change differente hyperparameters?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use our Q-table to play FrozenLake!\n",
    "- After many episodes, our Q-table can be used as a \"cheatsheet\" to play FrozenLake\".\n",
    "- By running this cell you can see our agent playing FrozenLake.\n",
    "- Evaluate how the reward evolves in each step and comment about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(1):\n",
    "    state = env.reset()[0]\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(params.max_steps):\n",
    "        \n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            # out = env.render()\n",
    "            # print(out)\n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "        \n",
    "        print(f'Step #{step+1}')\n",
    "        out = env.render()\n",
    "        print(out)\n",
    "\n",
    "\n",
    "\n",
    "        state = new_state\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional tests\n",
    "\n",
    "Increase the size of the map --> change the `map_size=4` parameter in `params`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
